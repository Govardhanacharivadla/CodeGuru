# LLM Provider Configuration
# Options: "ollama" (default, free, local) or "groq" (free API, faster)
LLM_PROVIDER=ollama

# Ollama Settings (Local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:7b
# Alternative models:
# - llama3.2:3b (faster, smaller)
# - codellama:7b (code-specialized)
# - qwen2.5-coder:14b (better quality, slower)

# Groq API Settings (Optional - Free Tier Available)
# Sign up at https://console.groq.com (no credit card required)
# Free tier: 30 requests/minute
GROQ_API_KEY=
GROQ_MODEL=llama-3.1-70b-versatile
# Alternative Groq models:
# - mixtral-8x7b-32768
# - llama-3.1-8b-instant (faster)

# Explanation Settings
EXPLANATION_DEPTH=all  # Options: simple, detailed, deep, all
GENERATE_DIAGRAMS=true
MAX_DIAGRAM_NODES=50

# Code Analysis Settings
MAX_FILE_SIZE_KB=500
SUPPORTED_LANGUAGES=python,javascript,typescript,rust

# Cache Settings (reduces repeated LLM calls)
ENABLE_CACHE=true
CACHE_TTL_SECONDS=3600

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
